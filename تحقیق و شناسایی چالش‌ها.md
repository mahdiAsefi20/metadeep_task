## چالش های multi-agent orchestration

بر اساس منابع ارائه شده، ساخت یک سیستم چندعامله برای تولید **استوری‌بورد ویدیو از متن زبان طبیعی** نیازمند تغییر از مسیرهای ساده و خطی (open-loop) به سیستم‌های پیچیده، **همکاری‌محور و دارای آگاهی از وضعیت** است.

در ادامه، ۷ چالش اصلی در ساخت چنین سیستمی با تمرکز روی **هم‌آرایی چندعامله (multi-agent orchestration)**، همراه با مثال‌های واقعی و راهکارهای فنی پیشنهادی آورده شده است:

---

### ۱. حفظ ثبات هویت شخصیت‌ها

- **چالش:** مدل‌های استاندارد ویدیویی اغلب هر شات را به صورت مستقل تولید می‌کنند، که منجر به **انحراف هویت** می‌شود؛ یعنی ویژگی‌های چهره، لباس یا موهای شخصیت بین شات‌ها تغییر می‌کند.
    
- **مثال واقعی:** مقالات **MAViS** و **CoAgent** نشان می‌دهند که بدون حافظه پایدار، سیستم‌ها حتی برای یک شخصیت، افرادی متفاوت تولید می‌کنند.
    
- **راهکارهای پیشنهادی:**
    
    - **مدیر حافظه سراسری (Global Context Manager):** یک عامل تخصصی برای تولید و ذخیره «پرتره‌های اصلی» (embeddings شخصیت) به عنوان حافظه پایدار بین شات‌ها.
        
    - **آموزش LoRA به‌صورت خودکار:** عامل مسئول نمونه‌گیری تصاویر چندنمایی از شات اولیه و آموزش مدل Low-Rank Adaptation برای استفاده همه عوامل تولید.
        
    - **لنگر (Anchor) گذاری زمانی دوطرفه:** برنامه‌ریز می‌تواند بین حالت‌هایی مانند **FLF2V** (First-and-Last-Frame to Video) تصمیم بگیرد، جایی که آخرین فریم شات قبلی و فریم هدف شات فعلی نقش anchor را بازی می‌کنند.
        

---

### ۲. مهندسی کانتکست در برابر بار حافظه

- **چالش:** ارائه کل تاریخچه تعامل به هر عامل باعث **پف کردن کانتکست و مصرف بالای حافظه** می‌شود، اما ارائه کانتکست ناکافی باعث از دست رفتن داستان کلی می‌شود.
    
- **مثال واقعی:** **OmniAgent** نشان داده که ساختارهای خطی ساده (یک کارگردان فراخوانی چند متخصص) نمی‌توانند «کانتکست درست در زمان درست» را به اشتراک بگذارند.
    
- **راهکارهای پیشنهادی:**
    
    - **بازیابی مبتنی بر ابرگراف:** اجازه دهید عوامل تنها وقتی کانتکست محلی کافی ندارند، با هم «جلسات تیمی» تشکیل دهند.
        
    - **حافظه چندسطحی سلسله‌مراتبی:** پیاده‌سازی سه سطح حافظه: **حافظه جهانی** (دانش پایدار)، **حافظه وظیفه‌ای** (اقلام میانی مانند استوری‌بورد) و **حافظه کاربر** (ترجیحات گذشته).
        
    - **جستجوی کانتکست بازگشتی:** برنامه‌ریزی برای جستجوی کانتکست مرتبط در گراف عامل‌ها تا عمق مشخص (D_{max}).
        

---

### ۳. هم‌آرایی بازخورد و برنامه‌ریزی مجدد (سیستم‌های بسته)

- **چالش:** اکثر سیستم‌ها از DAG استفاده می‌کنند که اجازه نمی‌دهد عوامل پایین‌دست در صورت شکست یک شات، به عوامل بالادست بازخورد دهند.
    
- **مثال واقعی:** چارچوب **OmniAgent** نشان می‌دهد که مسیرهای خطی شکننده هستند و تولیدکننده ویدیو نمی‌تواند مشکلات پیوستگی را به نویسنده اسکریپت منتقل کند.
    
- **راهکارهای پیشنهادی:**
    
    - **اجرای چرخه‌ای کنترل‌شده:** انتقال از DAG به گراف‌هایی با لبه‌های بازگشتی محدود و با «بودجه تلاش مجدد» (R_{max}=3).
        
    - **اصل ۳E (کاوش، بررسی، بهبود):** هر مرحله عامل در یک چرخه چندمرحله‌ای تولید-اصلاح قرار گیرد، با بازبینی صریح قبل از ادامه.
        
    - **تکرار آگاه از خطا:** حافظه عوامل شامل تلاش‌های قبلی (M_i) باشد تا اشتباهات تکراری رخ ندهد.
        

---

### ۴. سازگاری فنی بین اسکریپت و ابزارها

- **چالش:** داستان‌های زبان طبیعی اغلب شامل توالی‌های پیچیده هستند که ابزارهای فعلی قادر به تولید آن‌ها در یک شات ۵ ثانیه‌ای نیستند.
    
- **مثال واقعی:** **MAViS** نشان داده برخی اسکریپت‌ها «غیرقابل تولید» هستند، مثلاً متنی روی صفحه موبایل.
    
- **راهکارهای پیشنهادی:**
    
    - **راهنمای نوشتن اسکریپت:** عامل بازبین برای رعایت محدودیت‌ها مانند «یک عمل اصلی در هر شات».
        
    - **درج شات‌های انتقالی:** برنامه‌ریز شات‌های تکمیلی برای جلوگیری از تکرار پس‌زمینه و مشکلات دیگر اضافه کند.
        
    - **عامل بازبین سبک:** اطمینان از اینکه سرعت روایت با ژانر مطابقت دارد.
        

---

### ۵. مدیریت محیط ابزارهای ناهمگون

- **چالش:** تولید ویدیو نیازمند ترکیب مدل‌های AI (T2V, I2V, T2A) و ابزارهای غیرAI است، که مقیاس‌پذیری و بروزرسانی را سخت می‌کند.
    
- **مثال واقعی:** **UniVA** مدل‌های یکپارچه را ناکارآمد و غیرقابل انعطاف می‌داند.
    
- **راهکارهای پیشنهادی:**
    
    - **پروتکل زمینه مدل (MCP):** اجرای ماژول‌ها به‌صورت سرورهای جداگانه و امکان «plug-and-play».
        
    - **معماری Plan-Act دوعامله:** برنامه‌ریز برای تجزیه اهداف و عامل اجرا برای مدیریت جزئیات ابزارها.
        
    - **رده‌بندی ابزارها:** توابع را به «اتم‌ها» (یک‌منظوره) و «جریان‌ها» (ترکیبی) تقسیم کنیم.
        

---

### ۶. اعتبارسنجی تطابق بصری و معنایی

- **چالش:** معیارهای خودکار قادر به تشخیص «توهمات فیزیکی» یا ناسازگاری معنایی بین متن و ویدیو نیستند.
    
- **مثال واقعی:** **CoAgent** نشان می‌دهد که مدل‌ها گاهی براساس منطق عمومی، فیل را خاکستری می‌کنند حتی اگر اسکریپت «فیل صورتی» خواسته باشد.
    
- **راهکارهای پیشنهادی:**
    
    - **بازرسی چندمرحله‌ای:** عامل Verifier ابتدا **تطابق معنایی** و سپس **تطابق هویت بصری** را بررسی کند.
        
    - **بازبین‌های موازی:** چند بازبین به صورت موازی ارزیابی انجام دهند تا محدودیت یک مدل کاهش یابد.
        
    - **استراتژی تطبیقی تولید:** Verifier بتواند حالت تولید را تغییر دهد (مثلاً از T2V به FF2V) در صورت کشف ناسازگاری.
        

---

### ۷. انسجام و ریتم روایت جهانی

- **چالش:** تولید شات‌ها به صورت جداگانه اغلب منجر به ریتم و انسجام ضعیف روایت می‌شود، حتی اگر شات‌ها کیفیت بالایی داشته باشند.
    
- **مثال واقعی:** **OmniAgent** نشان می‌دهد که مسیرهای خطی قادر به حفظ «حس سینمایی» در ویدیوهای طولانی نیستند.
    
- **راهکارهای پیشنهادی:**
    
    - **ویرایشگر آگاه به ریتم:** عامل نهایی برای تطبیق ریتم و انتقال‌ها با الگوی تمپو هدف.
        
    - **برنامه‌ریزی استوری‌بورد-محور:** عامل «معمار ذهنی» برای تعیین تعداد شات‌ها (N) بر اساس مدت و ریتم قبل از تولید.
        
    - **ادغام متادیتای جهانی:** شامل ژانر، سبک و ریتم در JSON اولیه استوری‌بورد برای هدایت همه عوامل سیستم.
        

---

## چالش های انواع مدل های مختلف متن باز

بر اساس منابع ارائه‌شده، مدل‌های متن‌به‌ویدئو (T2V) متن‌باز با ترکیبی از چالش‌های بنیادین روبه‌رو هستند که تقریباً در همه‌ی معماری‌ها مشترک‌اند، و همچنین با محدودیت‌های متمایزی مواجه‌اند که بسته به اندازه‌ی مدل، راهبرد آموزش و کاربرد هدف (مثلاً شات‌های کوتاه در برابر روایت‌های بلند) متفاوت می‌شوند.

### ۱. چالش‌های بنیادینِ مشترک در اکثر مدل‌ها 
در میان تقریباً همه‌ی مدل‌های متن‌باز، چند مشکل پایدار همچنان اصلی‌ترین مانع برای توسعه‌دهندگان به شمار می‌آیند:

- **ثبات سوژه و هویت:** حفظ ظاهر یکسانِ یک کاراکتر (جلوگیری از «رانش هویتی») در شات‌های مختلف یا حتی درون یک شات، چالشی همگانی است. مدل‌هایی مانند Mora و AesopAgent در حفظ ثبات شخصیتِ اصلی کاستی‌هایی نشان داده‌اند.
    
- **انسجام زمانی و پایداری بصری:** بسیاری از مدل‌ها دچار «خطاهای بصری» مانند لرزش زمانی (temporal flickering) می‌شوند؛ جایی که ظاهر یا موقعیت اشیا بین فریم‌ها به‌طور غیرطبیعی جابه‌جا می‌شود.
    
- **درک قوانین فیزیکی:** شبیه‌سازی فیزیک دنیای واقعی—مانند سرریز شدن مایع از یک لیوان یا مکانیک راه‌رفتن انسان—برای بسیاری از مدل‌های متن‌باز ضعیف است.
    
- **تولید ویدئوهای بلندمدت:** اغلب مدل‌های کنونی «شات‌محور» هستند و کلیپ‌های کوتاه (معمولاً کمتر از ۱۰ ثانیه) تولید می‌کنند؛ زیرا مقیاس محاسبات و داده‌ها هنوز برای تولید سرتاسری ویدئوهای چنددقیقه‌ای با کیفیت بالا کافی نیست.
    

### ۲. چالش‌های متفاوت بر اساس معماری مدل
منابع نشان می‌دهند که انواع مختلف مدل‌ها بسته به طراحی زیربنایی خود با گلوگاه‌های فنی خاصی مواجه‌اند:

- **دیفیوژنِ صفر-شات در برابر مدل‌های ویدئوییِ آموزش‌دیده:** مدل‌هایی مانند Text2Video-Zero کیفیت ایستای بالایی را از ستون‌فقرات متن‌به‌تصویر (مثل Stable Diffusion) به ارث می‌برند، اما به‌دلیل نداشتن آموزش ویدئویی اختصاصی، با چالش‌های جدی در کیفیت زمانی روبه‌رو هستند. در مقابل، مدل‌های آموزش‌دیده مانند ModelScopeT2V جریان زمانی بهتری دارند، اما گاهی به پای جزئیات ایستای با وضوح بالای رویکردهای صفر-شات نمی‌رسند.
    
- **قوانین مقیاس‌پذیری:** تفاوت معناداری در «توانمندی» مدل‌ها بر اساس تعداد پارامترها دیده می‌شود. برای نمونه، یک مدل ۵ میلیارد پارامتری (مانند CogVideoX-5B) درک بهتری از قوانین فیزیکی نسبت به نسخه‌ی ۲ میلیاردی خود نشان می‌دهد؛ این امر حاکی از آن است که مدل‌های کوچک‌تر با «سقف توانمندی» خاصی مواجه‌اند که مدل‌های بزرگ‌تر کم‌کم از آن عبور می‌کنند.
    
- **ترنسفورمر در برابر بلوک‌های دیفیوژن:** انتخاب معماری بر نوع مصنوعات (artifact) اثر می‌گذارد. مثلاً Open-Sora Plan با استفاده از توجه کامل سه‌بعدی (3D Full Attention) حرکت را روان‌تر می‌کند، اما با چالش پیچیدگی درجه‌دو (quadratic complexity) مواجه است که برای عملی‌بودن به راهبردهایی مانند Skiparse Attention نیاز دارد.
    

### ۳. چالش‌های وظیفه‌محور و معنایی  
ارزیابی‌ها روی بنچمارک‌هایی مانند FETV و T2VBench نشان می‌دهد که توانایی مدل‌ها بسته به پیچیدگی معنایی پرامپت متفاوت است:

- **محتوای فرکانس بالا در برابر فرکانس پایین:** مدل‌ها معمولاً در تولید محتوای زمانیِ کم‌فرکانس—مانند «حرکات سیال» (ابرها، آب) یا «تغییرات نور»—عملکرد خوبی دارند. اما همگی در کنش‌های پر‌فرکانس شامل «انسان‌ها» یا «حرکات جنبشی پیچیده» (مثل مسابقه‌ی خودروها یا اعمال پیچیده‌ی انسانی) ضعیف‌اند، زیرا این موارد به جزئیات متراکم‌تری نیاز دارند.
    
- **کنترل ویژگی‌های خاص:** بیشتر مدل‌ها به‌راحتی دستوراتی مانند «رنگ» و «زاویه‌ی دوربین» را دنبال می‌کنند، اما تقریباً همگی در کنترل‌های معنایی پیچیده شکست می‌خورند؛ مانند کمیت دقیق (تولید دقیقاً سه شیء)، جهت حرکت، و ترتیب رویدادها (انجام کاری و سپس کاری دیگر).
    
- **هم‌ترازی با موسیقی:** مدل‌های تخصصی مانند AutoMV با چالش ویژه‌ی هم‌ترازسازی تصویر با ضرب‌آهنگ، ساختار و ترانه‌های موسیقی روبه‌رو هستند—بعدی از پویایی زمانی که مدل‌های عمومی T2V معمولاً به آن نمی‌پردازند.
    

### ۴. ارکستراسیون سیستمی به‌عنوان چالش نوظهور  
پژوهش‌های اخیر نشان می‌دهد که تمرکز چالش‌ها در حال انتقال از «مدل هسته‌ای» به «چارچوب ارکستراسیون» است. سامانه‌های چندعامله مانند MAViS و OmniAgent نشان می‌دهند که موفقیت در ویدئوهای بلند به نحوه‌ی هماهنگی عامل‌های نقش‌محور، اشتراک زمینه (context) و پشتیبانی از حلقه‌های بازخورد برای اصلاح تصمیم‌های بالادستی بستگی دارد. این امر چالش‌های تازه‌ای در مهندسی زمینه و منطق بازتولید ایجاد می‌کند که در مدل‌های تک‌مرحله‌ای و ساده وجود ندارند.

**جمع‌بندی:**  
در حالی که همه‌ی مدل‌ها در تلاش برای ثبات و واقع‌گرایی فیزیکی هم‌داستان‌اند، تفاوت‌ها در شیوه‌ی مدیریت کارایی محاسباتی، پیروی از دستورهای معنایی، و جریان روایی پیچیده آشکار می‌شود.

---
## منابع

Hu, Panwen, et al. “StoryAgent: Customized Storytelling Video Generation via Multi-Agent Collaboration.” _ArXiv.org_, 2024, arxiv.org/abs/2411.04925. Accessed 28 Jan. 2026.

Ji, Pengliang, et al. “T2VBench: Benchmarking Temporal Dynamics for Text-To-Video Generation.” _Thecvf.com_, 2024, pp. 5325–35, openaccess.thecvf.com/content/CVPR2024W/EvGenFM/html/Ji_T2VBench_Benchmarking_Temporal_Dynamics_for_Text-to-Video_Generation_CVPRW_2024_paper.html. Accessed 28 Jan. 2026.

Liang, Zhengyang, et al. “UniVA: Universal Video Agent towards Open-Source Next-Generation Video Generalist.” _ArXiv.org_, 2025, arxiv.org/abs/2511.08521. Accessed 28 Jan. 2026.

Lin, Bin, et al. “Open-Sora Plan: Open-Source Large Video Generation Model.” _ArXiv.org_, 2024, arxiv.org/abs/2412.00131.

Liu, Yuanxin, et al. “FETV: A Benchmark for Fine-Grained Evaluation of Open-Domain Text-To-Video Generation.” _Advances in Neural Information Processing Systems_, vol. 36, Dec. 2023, pp. 62352–87, proceedings.neurips.cc/paper_files/paper/2023/hash/c481049f7410f38e788f67c171c64ad5-Abstract-Datasets_and_Benchmarks.html. Accessed 28 Jan. 2026.

Tang, Xiaoxuan, et al. “AutoMV: An Automatic Multi-Agent System for Music Video Generation.” _ArXiv.org_, 2025, arxiv.org/abs/2512.12196. Accessed 28 Jan. 2026.

Wang, Qian, et al. “MAViS: A Multi-Agent Framework for Long-Sequence Video Storytelling.” _ArXiv.org_, 2025, arxiv.org/abs/2508.08487. Accessed 28 Jan. 2026.

Wei, Zheng, et al. “Hollywood Town: Long-Video Generation via Cross-Modal Multi-Agent Orchestration.” _ArXiv.org_, 2025, arxiv.org/abs/2510.22431. Accessed 28 Jan. 2026.

Zeng, Qinglin, et al. “CoAgent: Collaborative Planning and Consistency Agent for Coherent Video Generation.” _ArXiv.org_, 2025, arxiv.org/abs/2512.22536. Accessed 28 Jan. 2026.
